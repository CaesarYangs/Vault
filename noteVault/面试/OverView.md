
- 花粉项目相关

数据扩充 

二分类及结果可视化

AUC指标，精准率 召回率 F1 分数 准确率

CAM进行热力图可视化分析 cnn位置
分类网络只会提取到最具有判别性的特征。所以也有很多做法来提高分类网络的CAM精准度
**提取目标特征层，并进行加权融合获取激活图(CAM)。主要的区别在于上述step2叙述的特征层之间融合权重的选择上**

- Batch-Normalization 
解决cost function的错误

解决分布变化的问题 加入可训练的参数做归一化
本身可以代替其他的正则方式如dropout等


进阶预处理
共生矩阵用两个位置的像素的联合概率密度来定义，它不仅反映亮度的分布特征，也反映具有同样亮度或者接近亮度的像素之间的位置分布特性，是有关图像亮度变化的二阶统计特征。它是定义一组纹理特征的基础。

由于纹理是由灰度在空间位置上反复出现而形成的，因而在图像空间中像个某距离的两像素之间会存在一定的灰度关系，即图像中灰度的空间相关特性。灰度共生矩阵就是一种通过研究灰度的空间相关特性来描述纹理的常用方法。

- Resnet
多个神经网络聚合成一个块 加入恒等映射 从而解决神经网络退化问题
引入跳跃连接 加入恒等映射 解决神经网络退化
层数很深


BN初始化解决梯度消失和爆炸

常用的归一化方法：BN LayerNorm



- 激活函数
**Sigmoid、Tanh、ReLu，leaky relu**

- TF
训练技巧：要做梯度归一化,即算出来的梯度除以minibatch size

- Pytorch
模型微调 fine tuning





- 梯度消失、梯度爆炸
在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题

前面层上的梯度是来自后面层上梯度的乘积。

梯度消失：在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失。

梯度爆炸：在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度爆炸。

解决方法：
预训练 梯度剪切 更换激活函数 BN 残差网络 长短期网络LSTM


- 残差网络



- Kmeans
簇中所有数据的均值通常被称为这个簇的“质心”(centroids)。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值


- VGG 16优化
剪枝



- yolo

- yolo v2
BN # Batch Normalization 层和卷积层，池化层一样都是一个网络层。
用K-means

- yolo v3
残差网络

- Fast R-CNN SSD Yolo
1、卷积不再是对每个region proposal进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个region proposal分别做卷积，因为一张图像中有2000左右的region proposal，肯定相互之间的重叠率很高，因此产生重复计算。2、用ROI pooling进行特征的尺寸变换，因为全连接层的输入要求尺寸大小一样，因此不能直接把region proposal作为输入。3、将regressor放进网络一起训练，每个类别对应一个regressor，同时用softmax代替原来的SVM分类器。
