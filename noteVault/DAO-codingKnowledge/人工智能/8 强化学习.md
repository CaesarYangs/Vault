# 强化学习
通过试错来学习如何行为
没有模型，标签，说明或任何其它人提供的监督信号
- 基本思想：
	- 以回报的形式接受反馈
	- agent的效用使用回报函数来定义
	- 必须学习能最大化期望回报的行为
	- 所有的学习都是基于对样本结果的观察

- 我们不知道哪个状态是好的或者我们该采取哪个行动
- 必须实际试验了该行为和状态才能学到
- 假定所学的模型是正确的，用它来求解值

- 基于马可夫决策过程MDP：
不知道转移概率T或回报值R
必须通过实际实验了解该行为和状态才能学到

# 有模型强化学习
- 基于经验学习一个近似的模型

做几个实验得到数据->通过有限的数据来估计模型->根据这个模型来行动 类似于MDP 
# 无模型强化学习
## 被动强化学习
- 简化的任务：策略评价
	- 输入: 一个固定的策略 
	- 转移概率T(s,a,s’)未知
	- 回报 R(s,a,s’)未知
	- 目标: 学状态的值（学策略有多好，或学期望效用）
- 在这种情况下:
	- 关于该采取哪个行为并没有可选择的
	- 仅仅执行策略并从经验中学习
	- 这不是离线规划   在实际真实的世界中采取行动.
### 直接效用估计
- 目标: 为每个状态基于策略π计算值
- 思想: 对所观察到的样本值统一取平均
- **一个状态的效用是从该状态开始往后的期望总回报**

- 优点：
	- 易于理解
	- 不要求转移概率 T和回报 R已知
	- 它最终计算正确的平均值, 仅仅使用样本转移
- 缺点：
	- 它浪费了状态之间的联系的信息
	- 每个状态必须分别来学习
	- 学习时间长
### 基于样本的策略评价
尝试使用Bellman方程来进行策略评价
- 简化的Bellman ，为某个固定的策略不断地更新计算V 
### 时序差分学习
基本思想: 从每个经验中学习
值的时间差异学习
策略仍固定，仍做评价

TD value leaning是做策略评价的模型无关的方式, 模仿Bellman用允许的样本均值来更新
思想: 学 Q-values, not values 学Q值，而不是值

## 主动强化学习
- 完全的强化学习: 优化策略(类似于值迭代)
- 学习器做选择!
- 基本的折衷: exploration（探索） vs. exploitation（利用）
- 这不是离线规划!   你实际上在世界中采取行为并发现发生了什么…
### Q学习
Q值迭代
- 学状态-行为函数表Q（s,a）
- 对每个状态S，和行为A，它企图记忆在当前状态并采取行为A所获得的最大的回报

值迭代: 寻找后继的值 (depth-limited) 

- Q学习的性质：
**基于样本的 Q-value 迭代**
Amazing  result:  Q-learning  收敛到最优策略 – 即使你的行为是次优的
被称作off-policy  learning
注：
- 必须探索的足够
- 必须最终使得学习率足够小… 但不要太快减小它

### 探索与利用