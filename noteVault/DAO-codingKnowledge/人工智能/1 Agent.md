# Agent
### Outline
1. agent的本质
2. agent是否完美
3. 环境的多样性
4. 带来的各种agent分类
---
## 基本概念和定义
- 理性——理性Agent是人工智能方法的核心
- 智能
> **agent is an entity that perceives and acts**
> agent是一个能感知并行动的实体

- **Agent可以被视为：
通过传感器感知环境，并通过执行器对该环境产生作用的东西**
- Agent：人类Agent 机器人Agent  软件Agent

- 感知：
表示任何时刻Agent的感知输入
- 感知序列：
该Agent所收到的所有输入数据的完整历史

一般来说,Agent在任何给定时刻的行动依赖于到那个时刻为止该Agent的感知序列,而不是那些它感知不到的东西。
- 从数学角度：Agent函数描述了Agent的行为。它将任何一个给定的感知序列映射为一个实体或行动。

## 好的行为：理性的概念
> 理性Agent是做事正确的agent

**在任何情况下,判断是理性依赖于:**
1. 定义成功标准的性能度量
2. Agent对环境的先验知识
3. Agent可以完成的行动‘
4. Agent截止到此时的感知序列

### 全知，学习和自主性
理性使期望的性能最大化
全知和完美使得实际的性能最大化
- 理性的选择只依赖于到目前为止的感知序列
- 必须进行信息的收集(探测)
- 从感知到的数据中尽可能多的学习
- Agent依赖于先验知识而不是自身的感知信息,这样的Agent是脆弱的

==**理性的Agent选择行为能最大化它的期望性能**==


### 任务环境描述规范 PEAS
1. Permformance measure (性能度量)
2. Environment (环境)
3. Actuators (执行器)
4. Sensors (传感器)


### 任务环境的性质
1. 完全可观察和部分可观察
- 如果Agent的传感器在每个时间点上都能获取环境的完整状态,则说环境是完全可观察的
- 如果传感器能检测所有与行动决策相关的信息,则该任务环境是有效完全可观察的
2. 单Agent和多Agent
3. 确定的与随机的
- 如果环境的下一个状态完全取决于当前状态和Agent执行的动作,则我们说该环境是确定的;否则,是随机的。
4. 片段式的与延续式的
- 在片段式的环境中,Agent的经历被分成了一个个原子片段,每个片段中Agent感知信息并完成单个行动。
- 下一个片段不依赖于以前片段的行动
5. 静态的与动态的
- 如果环境在Agent计算的时候会变化,则我们称该Agent的环境是动态的。否则是静态的
- **动态的环境要求Agent要持续地做决策。**
6. 离散的与连续的
7. 已知的与未知

**最难处理的情况：部分可观察，多agent，随记，延续，动态，连续，未知的环境**


## Agent的结构：设计的智能
> Agent = 程序+**体系结构**

### Agent程序类别
1. 简单反射Agent 
	- 基于当前的感知选择行为,忽略历史感知
	- 拥有触发规则(if-then rules)
	- 智能来自于coder
	- 可以有一些随机的行为
2. 基于模型的反射Agent 
	改进：部分可观察环境 开始关注历史感知
	让Agent跟踪记录现在看不到的那部分世界,即Agent应该根据感知历史维持内部状态,从而至少反映出当前状态看不到的信息。
- 特点：
	- 根据感知和对世界的知识来做决策 世界状态基于感知输入被更新
	- 触发规则
	- 智能来自于两方面:一方面是存储的关于世界的知识,另一方面来自于coder
	- 可能会有随机的行为
3. 基于目标的Agent  ^54b89a
	- 仅仅知道当前的环境状态对决策而言不够,及除了当前的状态描述,Agent还需要目标信息来描述想要达到的状况。
	- Agent可以把这种信息和模型相结合,以选择能达到目标的行动。
4. 基于效用的Agent 
	- 是基于目标的Agent的升级版
	- 目标用一个效用函数来评价
	- 触发规则
	- 智能来自两方面,但是更少取决于coder
	- 不确定性是经常会发生的	
	**性能度量—给环境状态的任何给定序列赋一个值**
	**Agent的效用函数----是性能度量的内在化**
	==理性的基于效用的Agent选择使其期望效用最大化的行动。==
5. 学习Agent
	- 目标:提高性能
	- 有评判元件：通过反馈让学习元件判断agent是否足够好
	- 回报或惩罚
	- 性能元件：选择外部的行动
	- 学习元件：负责提高
	- 问题生成器：建议更多探索性的行动
	