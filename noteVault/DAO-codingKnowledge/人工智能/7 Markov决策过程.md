# Markov 决策
Non-Deterministic  Search 非确定性搜索
其假设未来的状态仅取决于当前的状态

## 序列式决策问题
- 随机网格世界

**网格世界中的Agent**
- Agent在每个时间步都接受回报（reward）
	每一步都接受小的 “LIVING”回报(  可以是负) § 在终止处获得大的回报(好或坏)
- 目标：最大化回报和

## Markov决策过程（MDP）
对完全可观察的环境，使用马可夫链转移模型和累加回报的这种序列式决策问题，称为马可夫决策过程（MDP）
- 解决序列式决策问题，Agent的效用依赖于一个决策序列
- 在每个状态，Agent都得到一个可正可负但肯定有限的回报，一个环境历史的效用值是对所得到的回报求和。
- MDPs  是非确定性搜索问题
解决方法之一：期望最大化搜索

- MDP是一个序列决策问题
	- Agent行为的结果具有随机性
	- 状态之间具有马科夫性质，即当前状态只和它的前一个状态有关，和更前的状态无关
	- 解：一个策略即在每个状态如何行动
	- 目标：最大化期望奖赏和

## Markov决策过程和Search问题的差别
- 主要差别：
搜索：后继函数Successor由当前状态产生后继状态
MDP:    由转移函数（概率分布）生成后继状态
- 次要差别：
由最小代价到最大回报
搜索：有一个目标测试；
MDP:  也有终止测试，但是终止可以是一个好的终止，也可能是一个坏的终止。

- 什么是MDP的解---Policies（策略）
- 搜索问题：路径
- MDP问题：策略policy

- 对MDP,我们希望一个最优的策略:  S  →  A
	- 一个策略就是对每个状态，给定一个行为
	- 一个最优策略是使得它的后继状态的期望效用最大的策略
	- 一个明确的策略定义了一个  reflex  agent（反射agent）

## 关于  MDPs的Markov性质
> “Markov”一般来说意味着给定当前的状态，未来的状态和过去的状态之间是独立的

> 对MDP来说，“Markov”意味着行为的结果仅仅依赖于当前的状态

> 这就类似于搜索，  successor  函数仅仅依赖于当前的状态

- Markov  Assumption（马科夫假设）
当前状态仅仅依赖于一个有限的固定数量的以前的状态

- 最优策略
是产生最高期望效用值的策略
风险和回报的平衡依赖于非终止状态R(s)的值而变化
需要仔细平衡风险与回报
## 序列状态效用值
- 时间上的效用
有限期的最优策略是非静态的；
无限期的最优策略是静态的；
- 序列的效用
- 折扣
一个合理的选择是最大化回报的和
合理的选择是更愿意立刻的回报，要好于后来的回报
1. 如何折扣：每下降一层,我们乘以折扣因子一次
2. 为何折扣：立刻的回报也许比后来的回报有更高的效用  也帮助我们的算法收敛

## Bellman方程

## 价值迭代
定理:  将收敛到唯一的最优值


## 策略迭代
如果一个行为明显优于其他行为，则某个状态的确定的效用值不需要很准确

策略迭代算法从某个初始策略π0开始，**交替执行**如下两步：
1. 策略评价：为每个固定的策略计算效用（并非最优效用！）直到收敛
2. 策略改进： 使用向前看一步（one-step lookahead）更新策略 用结果收敛的效用 (but not optimal!)作为未来的值

### 固定策略（策略评价）
Expectimax  trees通过计算所有行为的值，并取最大值的行为来计算最优值
如果我们固定策略, 则树将更加简单 – 每个状态仅仅一个行为

固定策略下的效用值
- 基本操作: 计算一个状态S在某个固定 策略（一般来说非最优）下的效用
- 定义一个状态在某个固定策略下的效用
从状态s，遵循某个策略会生成一条路径
则状态s遵循策略的效用定义为路径的折扣回报和

将贝尔曼方程转换为针对更新（类似于值迭代）

### 策略增强
现在第一步执行一个不同的行为
再遵循原来的行为

取一个新的策略。先换第一步的行为，看能不能得到更好的效果。
在当前的状态下首先采取行为a，然后遵循策略π的期望效用
如果效果更好，则将π更新为a

思想：rinsw and repeat

### e.g.：Dice游戏
希望选择一个行为使得后继状态的期望效用值最大

## 比较值迭代和策略迭代
### 值迭代
- 每次迭代更新值（隐含地）更新策略
- 我们并不跟踪策略，而是取得行为中期望效用最大的那个
### 策略迭代
- 我们做几步更新某个固定策略的效用 (每一步都很快因为我们只考虑一个行为，而非全部)
- 在策略被评价后, 一个新的策略被选择
- 新的策略将更好